<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Daniele Grandi </title> <meta name="author" content="Daniele Grandi"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%99%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://grndnl.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">resume </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Daniele</span> Grandi </h1> <p class="desc"><span title="International Phonetic Alphabet">[daˈɲɛ.lɛ ˈɡran.di]</span> <em>(Dani)</em> <br> <a href="https://www.research.autodesk.com/" rel="external nofollow noopener" target="_blank">Autodesk Research</a>. Data-driven design and Machine Learning.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/danielegrandi_profile2-480.webp 480w,/assets/img/danielegrandi_profile2-800.webp 800w,/assets/img/danielegrandi_profile2-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/danielegrandi_profile2.jpg?afdce41bb874dd895fdcc3b82339261f" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="danielegrandi_profile2.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>South Lake Tahoe, CA, 96150</p> </div> </div> <div class="clearfix"> <p>As a Principal Research Scientist at Autodesk Research, I’m working to further the machine understanding of mechanical design problems. I’m interested in leveraging machine learning, knowledge representation and reasoning, LLMs, knowledge graphs, and agentic AI to create the next generation of conceptual and detail design tools. In collaboration with UC Berkeley, CMU, Oregon State University, and MIT, we are researching methods to learn design best practices from CAD datasets, and releasing ML benchmarks to expose model biases in design tasks and foster industry and academic innovation.</p> <p>At Autodesk, I’ve also worked as a Design Engineer on generative design research prototypes. Previously, I worked as an engineering consultant for a metal AM startup, focusing on design, simulation, and optimization of assemblies for AM.</p> <p>I graduated from UC Berkeley with a Mechanical Engineering degree. I started working with 3D printers at Berkeley, where I founded the 3D Modeling Club. While additive manufacturing had been my main field of focus, I also enjoyed traditional manufacturing methods, applied to mechatronics projects. I enjoy getting my hands on all parts of a project, whether it involves design, coding, circuits, or fabrication.</p> <p>The first 3D printing startup that I joined was eucl3D, a Berkeley startup working with game developers to provide custom high-quality 3D printed collectibles.</p> <p>Through Project BAM, the second 3D printing startup I worked at, I learned more about metal additive manufacturing and became interested in design optimization.</p> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#5C6BC0"> <a href="https://en.wikipedia.org/wiki/Graph_neural_network" rel="external nofollow noopener" target="_blank">Graph Neural Network</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/material_prediction.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="material_prediction.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bian2022material" class="col-sm-8"> <div class="title">Material prediction for design automation using graph representation learning</div> <div class="author"> Shijie Bian, <em>Daniele Grandi</em>, Kaveh Hassani, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Elliot Sadler, Bodia Borijin, Axel Fernandes, Andrew Wang, Thomas Lu, Richard Otis, Nhut Ho, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Successful material selection is critical in designing and manufacturing products for design automation. Designers leverage their knowledge and experience to create high-quality designs by selecting the most appropriate materials through performance, manufacturability, and sustainability evaluation. Intelligent tools can help designers with varying expertise by providing recommendations learned from prior designs. To enable this, we introduce a graph representation learning framework that supports the material prediction of bodies in assemblies. We formulate the material selection task as a node-level prediction task over the assembly graph representation of CAD models and tackle it using Graph Neural Networks (GNNs). Evaluations over three experimental protocols performed on the Fusion 360 Gallery dataset indicate the feasibility of our approach, achieving a 0.75 top-3 micro-F1 score. The proposed framework can scale to large datasets and incorporate designers’ knowledge into the learning process. These capabilities allow the framework to serve as a recommendation system for design automation and a baseline for future work, narrowing the gap between human designers and intelligent design agents. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bian2022material</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Material prediction for design automation using graph representation learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bian, Shijie and Grandi, Daniele and Hassani, Kaveh and Sadler, Elliot and Borijin, Bodia and Fernandes, Axel and Wang, Andrew and Lu, Thomas and Otis, Richard and Ho, Nhut and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Design Engineering Technical Conferences and Computers and Information in Engineering Conference}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{86229}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{V03AT03A001}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{American Society of Mechanical Engineers}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#F1C40F"> <a href="https://en.wikipedia.org/wiki/Machine_learning" rel="external nofollow noopener" target="_blank">Machine Learning</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/joinable.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="joinable.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="willis2022joinable" class="col-sm-8"> <div class="title">Joinable: Learning bottom-up assembly of parametric cad joints</div> <div class="author"> Karl DD Willis, Pradeep Kumar Jayaraman, Hang Chu, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Yunsheng Tian, Yifei Li, Daniele Grandi, Aditya Sanghi, Linh Tran, Joseph G Lambourne, Armando Solar-Lezama, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Physical products are often complex assemblies combining a multitude of 3D parts modeled in computer-aided design (CAD) software. CAD designers build up these assemblies by aligning individual parts to one another using constraints called joints. In this paper we introduce JoinABLe, a learning-based method that assembles parts together to form joints. JoinABLe uses the weak supervision available in standard parametric CAD files without the help of object class labels or human guidance. Our results show that by making network predictions over a graph representation of solid models we can outperform multiple baseline methods with an accuracy (79.53%) that approaches human performance (80%). Finally, to support future research we release the Fusion 360 Gallery assembly dataset, containing assemblies with rich information on joints, contact surfaces, holes, and the underlying assembly graph structure.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">willis2022joinable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Joinable: Learning bottom-up assembly of parametric cad joints}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Willis, Karl DD and Jayaraman, Pradeep Kumar and Chu, Hang and Tian, Yunsheng and Li, Yifei and Grandi, Daniele and Sanghi, Aditya and Tran, Linh and Lambourne, Joseph G and Solar-Lezama, Armando and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{15849--15860}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#606060"> <a href="https://www.uspto.gov/" rel="external nofollow noopener" target="_blank">Patent</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/" class="preview z-depth-1 rounded" width="100%" height="auto" alt="" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="willis2022conversion" class="col-sm-8"> <div class="title">Conversion of geometry to boundary representation with facilitated editing for computer aided design and 2.5-axis subtractive manufacturing</div> <div class="author"> Karl Darcy Daniel Willis, Nigel Jed Wesley Morris, Andreas Linas Bastian, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Adrian Adam Thomas Butscher, Daniele Grandi, Suguru Furuta, Joseph George Lambourne, Tristan Ward Barback, Martin Cvetanov Marinov, Marco Amagliani, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> Sep 2022 </div> <div class="periodical"> US Patent 11,455,435 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p></p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@book</span><span class="p">{</span><span class="nl">willis2022conversion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Conversion of geometry to boundary representation with facilitated editing for computer aided design and 2.5-axis subtractive manufacturing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Willis, Karl Darcy Daniel and Morris, Nigel Jed Wesley and Bastian, Andreas Linas and Butscher, Adrian Adam Thomas and Grandi, Daniele and Furuta, Suguru and Lambourne, Joseph George and Barback, Tristan Ward and Marinov, Martin Cvetanov and Amagliani, Marco and others}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{US Patent 11,455,435}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#E91E63"> <a href="https://en.wikipedia.org/wiki/Large_language_model" rel="external nofollow noopener" target="_blank">LLM</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/conceptual.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="conceptual.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2023conceptual" class="col-sm-8"> <div class="title">Conceptual design generation using large language models</div> <div class="author"> Kevin Ma, <em>Daniele Grandi</em>, Christopher McComb, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Kosa Goucher-Lambert' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Concept generation is a creative step in the conceptual design phase, where designers often turn to brainstorming, mindmapping, or crowdsourcing design ideas to complement their own knowledge of the domain. Recent advances in natural language processing (NLP) and machine learning (ML) have led to the rise of Large Language Models (LLMs) capable of generating seemingly creative outputs from textual prompts. The success of these models has led to their integration and application across a variety of domains, including art, entertainment, and other creative work. In this paper, we leverage LLMs to generate solutions for a set of 12 design problems and compare them to a baseline of crowdsourced solutions. We evaluate the differences between generated and crowdsourced design solutions through multiple perspectives, including human expert evaluations and computational metrics. Expert evaluations indicate that the LLM-generated solutions have higher average feasibility and usefulness while the crowdsourced solutions have more novelty. We experiment with prompt engineering and find that leveraging few-shot learning can lead to the generation of solutions that are more similar to the crowdsourced solutions. These findings provide insight into the quality of design solutions generated with LLMs and begins to evaluate prompt engineering techniques that could be leveraged by practitioners to generate higher-quality design solutions synergistically with LLMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ma2023conceptual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Conceptual design generation using large language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Kevin and Grandi, Daniele and McComb, Christopher and Goucher-Lambert, Kosa}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Design Engineering Technical Conferences and Computers and Information in Engineering Conference}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{87349}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{V006T06A021}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{American Society of Mechanical Engineers}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#E91E63"> <a href="https://en.wikipedia.org/wiki/Large_language_model" rel="external nofollow noopener" target="_blank">LLM</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/whats_in_a_name.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="whats_in_a_name.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="meltzer2024s" class="col-sm-8"> <div class="title">What’s in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models Through User-Provided Names in Computer Aided Design Files</div> <div class="author"> Peter Meltzer, Joseph G Lambourne, and <em>Daniele Grandi</em> </div> <div class="periodical"> <em>Journal of Computing and Information Science in Engineering</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This work investigates the value in the natural language part and document names users provide when they create CAD models. In a first step towards multi-modal text-CAD learning, our results show that Large Language Models are able to leverage the noisy text data to predict part-part and part-whole relationships, with direct applications in automations and recommendations for part re-use, auto-complete, assembly categorizations, smart tool suggestions and library part recommendations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">meltzer2024s</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{What’s in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models Through User-Provided Names in Computer Aided Design Files}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Meltzer, Peter and Lambourne, Joseph G and Grandi, Daniele}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Computing and Information Science in Engineering}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{011002}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{American Society of Mechanical Engineers}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#5C6BC0"> <a href="https://en.wikipedia.org/wiki/Graph_neural_network" rel="external nofollow noopener" target="_blank">Graph Neural Network</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/hg-cad.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hg-cad.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bian2024hg" class="col-sm-8"> <div class="title">HG-CAD: hierarchical graph learning for material prediction and recommendation in computer-aided design</div> <div class="author"> Shijie Bian, <em>Daniele Grandi</em>, Tianyang Liu, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Pradeep Kumar Jayaraman, Karl Willis, Elliot Sadler, Bodia Borijin, Thomas Lu, Richard Otis, Nhut Ho, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>Journal of Computing and Information Science in Engineering</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>To support intelligent computer-aided design (CAD), we introduce a machine learning architecture, namely HG-CAD, that recommends assembly body material through joint learning of body and assembly-level features using a hierarchical graph representation. Specifically, we formulate the material prediction and recommendation process as a node-level classification task over a novel hierarchical graph representation of CAD models, with a low-level graph capturing the body geometry, a high-level graph representing the assembly topology, and a batch-level mask randomization enabling contextual awareness. This enables our network to aggregate geometric and topological features from both the body and assembly levels, leading to competitive performance. Qualitative and quantitative evaluation of the proposed architecture on the Fusion 360 Gallery Assembly Dataset demonstrates the feasibility of our approach, outperforming selected computer vision and human baselines while showing promise in application scenarios. The proposed HG-CAD architecture that unifies the processing, encoding, and joint learning of multi-modal CAD features indicates the potential to serve as a recommendation system for design automation and a baseline for future work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bian2024hg</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HG-CAD: hierarchical graph learning for material prediction and recommendation in computer-aided design}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bian, Shijie and Grandi, Daniele and Liu, Tianyang and Jayaraman, Pradeep Kumar and Willis, Karl and Sadler, Elliot and Borijin, Bodia and Lu, Thomas and Otis, Richard and Ho, Nhut and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Computing and Information Science in Engineering}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{011007}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{American Society of Mechanical Engineers}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#3498DB"> <a href="https://en.wikipedia.org/wiki/Data_set" rel="external nofollow noopener" target="_blank">Dataset</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/designqa.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="designqa.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="doris2025designqa" class="col-sm-8"> <div class="title">Designqa: A multimodal benchmark for evaluating large language models’ understanding of engineering documentation</div> <div class="author"> Anna C Doris, <em>Daniele Grandi</em>, Ryan Tomich, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Md Ferdous Alam, Mohammadmehdi Ataei, Hyunmin Cheong, Faez Ahmed' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Journal of Computing and Information Science in Engineering</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This research introduces DesignQA, a novel benchmark aimed at evaluating the proficiency of multimodal large language models (MLLMs) in comprehending and applying engineering requirements in technical documentation. Developed with a focus on real-world engineering challenges, DesignQA uniquely combines multimodal data-including textual design requirements, CAD images, and engineering drawings-derived from the Formula SAE student competition. Different from many existing MLLM benchmarks, DesignQA contains document-grounded visual questions where the input image and input document come from different sources. The benchmark features automatic evaluation metrics and is divided into segments-Rule Comprehension, Rule Compliance, and Rule Extraction-based on tasks that engineers perform when designing according to requirements. We evaluate state-of-the-art models like GPT4 and LLaVA against the benchmark, and our study uncovers the existing gaps in MLLMs’ abilities to interpret complex engineering documentation. Key findings suggest that while MLLMs demonstrate potential in navigating technical documents, substantial limitations exist, particularly in accurately extracting and applying detailed requirements to engineering designs. This benchmark sets a foundation for future advancements in AI-supported engineering design processes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">doris2025designqa</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Designqa: A multimodal benchmark for evaluating large language models’ understanding of engineering documentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Doris, Anna C and Grandi, Daniele and Tomich, Ryan and Alam, Md Ferdous and Ataei, Mohammadmehdi and Cheong, Hyunmin and Ahmed, Faez}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Computing and Information Science in Engineering}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{25}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{021009}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{American Society of Mechanical Engineers}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#E91E63"> <a href="https://en.wikipedia.org/wiki/Large_language_model" rel="external nofollow noopener" target="_blank">LLM</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/elicitron.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="elicitron.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ataei2024elicitron" class="col-sm-8"> <div class="title">Elicitron: An LLM agent-based simulation framework for design requirements elicitation</div> <div class="author"> Mohammadmehdi Ataei, Hyunmin Cheong, <em>Daniele Grandi</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Ye Wang, Nigel Morris, Alexander Tessier' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2404.16045</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Requirements elicitation, a critical, yet time-consuming and challenging step in product development, often fails to capture the full spectrum of user needs. This may lead to products that fall short of expectations. This paper introduces a novel framework that leverages Large Language Models (LLMs) to automate and enhance the requirements elicitation process. LLMs are used to generate a vast array of simulated users (LLM agents), enabling the exploration of a much broader range of user needs and unforeseen use cases. These agents engage in product experience scenarios, through explaining their actions, observations, and challenges. Subsequent agent interviews and analysis uncover valuable user needs, including latent ones. We validate our framework with three experiments. First, we explore different methodologies for diverse agent generation, discussing their advantages and shortcomings. We measure the diversity of identified user needs and demonstrate that context-aware agent generation leads to greater diversity. Second, we show how our framework effectively mimics empathic lead user interviews, identifying a greater number of latent needs than conventional human interviews. Third, we showcase that LLMs can be used to analyze interviews, capture needs, and classify them as latent or not. Our work highlights the potential of using LLM agents to accelerate early-stage product development, reduce costs, and increase innovation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ataei2024elicitron</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Elicitron: An LLM agent-based simulation framework for design requirements elicitation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ataei, Mohammadmehdi and Cheong, Hyunmin and Grandi, Daniele and Wang, Ye and Morris, Nigel and Tessier, Alexander}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2404.16045}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#E91E63"> <a href="https://en.wikipedia.org/wiki/Large_language_model" rel="external nofollow noopener" target="_blank">LLM</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/llm-material-selection.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="llm-material-selection.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="grandi2025evaluating" class="col-sm-8"> <div class="title">Evaluating large language models for material selection</div> <div class="author"> <em>Daniele Grandi</em>, Yash Patawari Jain, Allin Groom, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Brandon Cramer, Christopher McComb' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Journal of Computing and Information Science in Engineering</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Material selection is a crucial step in conceptual design due to its significant impact on the functionality, aesthetics, manufacturability, and sustainability impact of the final product. This study investigates the use of Large Language Models (LLMs) for material selection in the product design process and compares the performance of LLMs against expert choices for various design scenarios. By collecting a dataset of expert material preferences, the study provides a basis for evaluating how well LLMs can align with expert recommendations through prompt engineering and hyperparameter tuning. The divergence between LLM and expert recommendations is measured across different model configurations, prompt strategies, and temperature settings. This approach allows for a detailed analysis of factors influencing the LLMs’ effectiveness in recommending materials. The results from this study highlight two failure modes, and identify parallel prompting as a useful prompt-engineering method when using LLMs for material selection. The findings further suggest that, while LLMs can provide valuable assistance, their recommendations often vary significantly from those of human experts. This discrepancy underscores the need for further research into how LLMs can be better tailored to replicate expert decision-making in material selection. This work contributes to the growing body of knowledge on how LLMs can be integrated into the design process, offering insights into their current limitations and potential for future improvements.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">grandi2025evaluating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Evaluating large language models for material selection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Grandi, Daniele and Jain, Yash Patawari and Groom, Allin and Cramer, Brandon and McComb, Christopher}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Computing and Information Science in Engineering}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{25}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{021004}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{American Society of Mechanical Engineers}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%67%6E%72%64%6E%6C@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/grndnl" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=X0qp478AAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.research.autodesk.com/people/daniele-grandi/" title="Work" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-briefcase"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Daniele Grandi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-J3WHPVT4RF"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-J3WHPVT4RF');
  </script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>